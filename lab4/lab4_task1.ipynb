{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03135afc-7d00-481a-aa52-9320d7c508d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfd25cc9-9a00-4c4d-88b3-248dc55c64c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you need to drink wine?\tВам потрібно випити вина?\tCC-BY 2.0 (France) Attribution: tatoeba.org #1122357 (cntrational) & #5763235 (deniko)\n",
      "Tom gave Mary a ring.\tТом дав Мері обручку.\tCC-BY 2.0 (France) Attribution: tatoeba.org #5822260 (CK) & #7499658 (deniko)\n",
      "Tom confided in Mary.\tТом поклався на Мері.\tCC-BY 2.0 (France) Attribution: tatoeba.org #5105749 (CK) & #5805055 (deniko)\n"
     ]
    }
   ],
   "source": [
    "with open('ukr.txt', 'r', encoding = 'utf-8') as f:\n",
    "  lines = f.read().split('\\n')[:-1]\n",
    "\n",
    "for _ in range(3):\n",
    "    print(random.choice(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "089730be-f13f-4db4-98a0-73519cb74631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Tom hasn't yet paid.\", '[s] Том ще не платив. [e]')\n",
      "('I think Tom is a little shy.', \"[s] Мені здається, Том трохи сором'язливий. [e]\")\n",
      "('I worked in Boston.', '[s] Я працював у Бостоні. [e]')\n",
      "(\"Look at her. She's beautiful.\", '[s] Подивись на неї. Вона прекрасна. [e]')\n",
      "(\"It's quiet.\", '[s] Вона тиха. [e]')\n"
     ]
    }
   ],
   "source": [
    "text_pairs = []\n",
    "for line in lines:\n",
    "  eng, ukr, _ = line.split('\\t')\n",
    "  ukr = '[s] ' + ukr + ' [e]'\n",
    "  text_pairs.append((eng, ukr))\n",
    "\n",
    "for t in range(5):\n",
    "    print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e004bca-c3ec-4e3b-b910-3ac10d72fd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all pairs: 50000\n",
      "train pairs: 35000\n",
      "validation pairs: 7500\n",
      "test pairs: 7500\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(text_pairs)\n",
    "text_pairs = text_pairs[:50000]\n",
    "num_val = int(0.15 * len(text_pairs))\n",
    "num_train = len(text_pairs) - 2 * num_val\n",
    "train_pairs = text_pairs[:num_train]\n",
    "val_pairs = text_pairs[num_train : num_train + num_val]\n",
    "test_pairs = text_pairs[num_train + num_val :]\n",
    "\n",
    "print(f'all pairs: {len(text_pairs)}')\n",
    "print(f'train pairs: {len(train_pairs)}')\n",
    "print(f'validation pairs: {len(val_pairs)}')\n",
    "print(f'test pairs: {len(test_pairs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f0f5776-1d71-46ed-a6f9-9ef85d1413bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_chars = string.punctuation.replace('[', '')\n",
    "strip_chars = strip_chars.replace(']', '')\n",
    "\n",
    "vocabulary_size = 15000\n",
    "sequence_length = 20\n",
    "batch_size = 64\n",
    "\n",
    "def ukr_standardization(input_string):\n",
    "    return tf.strings.regex_replace(tf.strings.lower(input_string), '[%s]' % re.escape(strip_chars), '')\n",
    "\n",
    "eng_vector = keras.layers.TextVectorization(\n",
    "    max_tokens=vocabulary_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "ukr_vector = keras.layers.TextVectorization(\n",
    "    max_tokens=vocabulary_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=ukr_standardization,\n",
    ")\n",
    "train_eng = [pair[0] for pair in train_pairs]\n",
    "train_ukr = [pair[1] for pair in train_pairs]\n",
    "eng_vector.adapt(train_eng)\n",
    "ukr_vector.adapt(train_ukr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "915abea1-3ace-432c-aa31-fb59698245ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6422\n",
      "[np.str_('do'), np.str_('have'), np.str_('im'), np.str_('mary'), np.str_('was'), np.str_('dont'), np.str_('he'), np.str_('me'), np.str_('in'), np.str_('it')]\n",
      "15000\n",
      "[np.str_('на'), np.str_('Мері'), np.str_('ти'), np.str_('Тома'), np.str_('я'), np.str_('з'), np.str_('у'), np.str_('Це'), np.str_('У'), np.str_('мене')]\n"
     ]
    }
   ],
   "source": [
    "for v in [eng_vector, ukr_vector]:\n",
    "  print(len(v.get_vocabulary()))\n",
    "  print(v.get_vocabulary()[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3e75be7-3620-4eaa-8344-96116d49ac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(eng, ukr):\n",
    "  eng = eng_vector(eng)\n",
    "  ukr = ukr_vector(ukr)\n",
    "  return ({ \"encoder_inputs\": eng, \"decoder_inputs\": ukr[:, :-1]}, ukr[:, 1:])\n",
    "\n",
    "def make_dataset(pairs):\n",
    "  eng_texts, ukr_texts = zip(*pairs)\n",
    "  eng_texts = list(eng_texts)\n",
    "  ukr_texts = list(ukr_texts)\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((eng_texts, ukr_texts))\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  dataset = dataset.map(format_dataset)\n",
    "  return dataset.cache().shuffle(2048).prefetch(16)\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e96fe01-1916-4be4-a3fc-e97bf070b137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder inputs shape: (64, 20)\n",
      "decoder inputs shape: (64, 20)\n",
      "targets shape: (64, 20)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'encoder inputs shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'decoder inputs shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    print(f\"targets shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1e07e6b-2e29-47bc-aea8-7cd325c5d30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(keras.layers.Layer):\n",
    "  def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "    super(TransformerEncoder, self).__init__()\n",
    "    self.attention = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "    self.dense_proj = tf.keras.Sequential([keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim)])\n",
    "    self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.dropout1 = keras.layers.Dropout(rate)\n",
    "    self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "  def call(self, inputs, training):\n",
    "    attn_output = self.attention(inputs, inputs)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(inputs + attn_output)\n",
    "    ffn_output = self.dense_proj(out1)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e8e6c51-bcd5-42ed-8ea3-6ce4ce90cb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(keras.layers.Layer):\n",
    "  def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "    super(TransformerDecoder, self).__init__()\n",
    "    self.attention1 = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "    self.attention2 = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "    self.dense_proj = tf.keras.Sequential([keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim)])\n",
    "    self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.dropout1 = keras.layers.Dropout(rate)\n",
    "    self.dropout2 = keras.layers.Dropout(rate)\n",
    "    self.dropout3 = keras.layers.Dropout(rate)\n",
    "\n",
    "  def call(self, inputs, enc_output, training):\n",
    "    attn_output1 = self.attention1(inputs, inputs)\n",
    "    attn_output1 = self.dropout1(attn_output1, training=training)\n",
    "    out1 = self.layernorm1(inputs + attn_output1)\n",
    "    attn_output2 = self.attention2(out1, enc_output)\n",
    "    attn_output2 = self.dropout2(attn_output2, training=training)\n",
    "    out2 = self.layernorm2(out1 + attn_output2)\n",
    "    ffn_output = self.dense_proj(out2)\n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    return self.layernorm3(out2 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb5a1dae-b709-403e-8632-33a75cd50d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:204: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 256\n",
    "latent_dim = 2048\n",
    "num_heads = 8\n",
    "\n",
    "# Вхідні дані для енкодера\n",
    "encoder_inputs = keras.layers.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "x = keras.layers.Embedding(input_dim=vocabulary_size, output_dim=embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, num_heads, latent_dim)(x, training=True)\n",
    "\n",
    "# Вхідні дані для декодера\n",
    "decoder_inputs = keras.layers.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "x = keras.layers.Embedding(input_dim=vocabulary_size, output_dim=embed_dim)(decoder_inputs)\n",
    "\n",
    "# Підключення декодера до виходів енкодера\n",
    "x = TransformerDecoder(embed_dim, num_heads, latent_dim)(x, encoder_outputs, training=True)\n",
    "decoder_outputs = keras.layers.Dense(vocabulary_size, activation=\"softmax\")(x)\n",
    "\n",
    "# Створення моделі\n",
    "transformer = keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee085c3e-4cb0-4d5e-8a79-0268252d0b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['encoder_inputs', 'decoder_inputs']. Received: the structure of inputs={'encoder_inputs': '*', 'decoder_inputs': '*'}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 903ms/step - accuracy: 0.7304 - loss: 2.4729 - val_accuracy: 0.7678 - val_loss: 1.5149\n",
      "Epoch 2/5\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m492s\u001b[0m 899ms/step - accuracy: 0.7685 - loss: 1.5252 - val_accuracy: 0.7712 - val_loss: 1.4156\n",
      "Epoch 3/5\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m488s\u001b[0m 891ms/step - accuracy: 0.7689 - loss: 1.4428 - val_accuracy: 0.7749 - val_loss: 1.3663\n",
      "Epoch 4/5\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m490s\u001b[0m 895ms/step - accuracy: 0.7786 - loss: 1.2959 - val_accuracy: 0.7769 - val_loss: 1.3333\n",
      "Epoch 5/5\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m486s\u001b[0m 888ms/step - accuracy: 0.7806 - loss: 1.2295 - val_accuracy: 0.7803 - val_loss: 1.2763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2d0ee4d5910>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "transformer.compile(\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae1439bd-276c-4715-8ee6-b435f8fe0469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--input: Do you think Tom will call?\n",
      "--output: [s] Том [e]\n",
      "--input: Did you speak to Tom yesterday?\n",
      "--output: [s] Том [e]\n",
      "--input: I'm very glad to see you.\n",
      "--output: [s] Том [e]\n"
     ]
    }
   ],
   "source": [
    "ukr_vocab = ukr_vector.get_vocabulary()\n",
    "ukr_index_lookup = dict(zip(range(len(ukr_vocab)), ukr_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "  tokenized_input_sentence = eng_vector([input_sentence])\n",
    "  decoded_sentence = \"[s]\"\n",
    "  for i in range(max_decoded_sentence_length):\n",
    "    tokenized_target_sentence = ukr_vector([decoded_sentence])[:, :-1]\n",
    "    predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
    "    sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "    sampled_token = ukr_index_lookup[sampled_token_index]\n",
    "    decoded_sentence += \" \" + sampled_token\n",
    "    if sampled_token == \"[e]\":\n",
    "      break\n",
    "  return decoded_sentence\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(3):\n",
    "  input_sentence = random.choice(test_eng_texts)\n",
    "  translated = decode_sequence(input_sentence)\n",
    "  print(f'--input: {input_sentence}')\n",
    "  print(f'--output: {translated}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820230a1-3014-4a44-9bd3-4d5f605db938",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
